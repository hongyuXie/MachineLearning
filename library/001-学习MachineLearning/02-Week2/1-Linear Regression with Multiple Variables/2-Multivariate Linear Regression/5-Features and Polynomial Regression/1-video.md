# 特征量和多项式回归
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/02-Week2/1 Linear Regression with Multiple Variables/5-Features and Polynomial Regression.mp4" type="video/mp4">
</video>
## 中文
### 举例说明什么是多项式回归
![举例说明什么是多项式回归](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/22-举例说明什么是多项式回归.jpg)
你现在了解了多变量的线性回归 在本段视频中 我想告诉你 一些用来 选择特征的方法以及 如何得到不同的学习算法 当选择了合适的特征后 这些算法往往是非常有效的 另外 我也想 给你们讲一讲多项式回归 它使得你们能够使用 线性回归的方法来拟合 非常复杂的函数 甚至是非线性函数 以预测房价为例 假设你有两个特征 分别是房子临街的宽度和垂直宽度 这就是我们想要卖出的房子的图片 临街宽度 被定义为这个距离 其实就是它的宽度 或者说是 你拥有的土地的宽度 如果这块地都是你的的话 而这所房子的 纵向深度就是 你的房子的深度 这是正面的宽度 这是深度 我们称之为临街宽度和纵深 你可能会 像这样 建立一个 线性回归模型 其中临街宽度 是你的第一个特征x1 纵深是你的第二个 特征x2 但当我们在 运用线性回归时 你不一定非要直接用 给出的 x1 和 x2 作为特征 其实你可以自己创造新的特征 因此 如果我要预测 房子的价格 我真正要需做的 也许是 确定真正能够决定 我房子大小 或者说我土地大小 的因素是什么 因此 我可能会创造一个新的特征 我称之为 x 它是临街宽度与纵深的乘积 这是一个乘法符号 它是临街宽度与纵深的乘积 这得到的就是我拥有的土地的面积 然后 我可以把 假设选择为 使其只使用 一个特征 也就是我的 土地的面积 对吧？ 由于矩形面积的 计算方法是 矩形长和宽相乘 因此 这取决于 你从什么样的角度 去审视一个特定的问题 而不是 只是直接去使用临街宽度和纵深 这两个我们只是碰巧在开始时 使用的特征 有时 **通过定义 新的特征** 你确实会得到一个更好的模型 与选择特征的想法 密切相关的一个概念 **被称为多项式回归(polynomial regression)**    
### 举例说明多项式回归以及注意特征缩放问题
![举例说明多项式回归以及注意特征缩放问题](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/23-举例说明多项式回归以及注意特征缩放问题.jpg)
比方说 你有这样一个住房价格的数据集 为了拟合它 可能会有多个不同的模型供选择 其中一个你可以选择的是像这样的二次模型 因为直线似乎并不能很好地拟合这些数据 因此 也许你会想到 用这样的二次模型去拟合数据 你可能会考量 是关于价格的一个二次函数 也许这样做 会给你一个 像这样的拟合结果 但是 然后你可能会觉得 二次函数的模型并不好用 因为 一个二次函数最终 会降回来 而我们并不认为 房子的价格在高到一定程度后 会下降回来 因此 也许我们会 选择一个不同的多项式模型 并转而选择使用一个 三次函数 在这里 现在我们有了一个三次的式子 我们用它进行拟合 我们可能得到这样的模型 也许这条绿色的线 对这个数据集拟合得更好 因为它不会在最后下降回来 那么 我们到底应该如何将模型与我们的数据进行拟合呢？ 使用多元 线性回归的方法 我们可以 通过将我们的算法做一个非常简单的修改来实现它 按照我们以前假设的形式 我们知道如何对 这样的模型进行拟合 其中 ħθ(x) 等于 θ0 +θ1×x1 + θ2×x2 + θ3×x3 那么 如果我们想 拟合这个三次模型 就是我用绿色方框框起来的这个 现在我们讨论的是 为了预测一栋房子的价格 我们用 θ0 加 θ1 乘以房子的面积 加上 θ2 乘以房子面积的平方 因此 这个式子与那个式子是相等的 然后再加 θ3 乘以 房子面积的立方 为了将这两个定义 互相对应起来 为了做到这一点 我们自然想到了 将 x1 特征设为 房子的面积 将第二个特征 x2 设为 房屋面积的平方 将第三个特征 x3 设为 房子面积的立方 那么 仅仅通过将 这三个特征这样设置 然后再应用线性回归的方法 我就可以拟合 这个模型 并最终 将一个三次函数拟合到我的数据上 我还想再说一件事 那就是 如果你像这样选择特征 那么特征的归一化 就变得更重要了 因此 如果 房子的大小范围在 1到1000之间 那么 比如说 从1到1000平方尺 那么 房子面积的平方 的范围就是 一到一百万 也就是 1000的平方 而你的第三个特征 x的立方 抱歉 你的第三个特征 x3 它是房子面积的 立方 范围会扩大到 1到10的9次方 因此 这三个特征的范围 有很大的不同 因此 如果你使用梯度下降法 应用特征值的归一化是非常重要的 这样才能将他们的 值的范围变得具有可比性
### 举例说明如何进行特征选择
![举例说明如何进行特征选择](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/24-举例说明如何进行特征选择.jpg)
最后 这里是最后一个例子 关于如何使你 真正选择出要使用的特征 此前我们谈到 一个像这样的二次模型 并不是理想的 因为 你知道 也许一个二次模型能很好地拟合 这个数据 但二次 函数最后会下降 这是我们不希望的 就是住房价格往下走 像预测的那样 出现房价的下降 但是 除了转而 建立一个三次模型以外 你也许有其他的选择 特征的方法 这里有很多可能的选项 但是给你另外一个 合理的选择的例子 另一种合理的选择 可能是这样的 一套房子的价格是 θ0 加 θ1 乘以 房子的面积 然后 加 θ2 乘以房子面积的平方根 可以吧？ 平方根函数是 这样的一种函数 也许θ1 θ2 θ3 中会有一些值 会捕捉到这个模型 从而使得这个曲线看起来 是这样的 趋势是上升的 但慢慢变得 平缓一些 而且永远不会 下降回来 因此 通过深入地研究 在这里我们研究了平方根 函数的形状 并且 更深入地了解了选择不同特征时数据的形状 有时可以得到更好的模型 在这段视频中 我们探讨了多项式回归 也就是 如何将一个 多项式 如一个二次函数 或一个三次函数拟合到你的数据上 除了这个方面 我们还讨论了 在使用特征时的选择性 例如 我们不使用 房屋的临街宽度和纵深 也许 你可以 把它们乘在一起 从而得到 房子的土地面积这个特征 实际上 这似乎有点 难以抉择 这里有这么多 不同的特征选择 我该如何决定使用什么特征呢 在之后的课程中 我们将 探讨一些算法 它们能够 自动选择要使用什么特征 因此 你可以使用一个算法 观察给出的数据 并自动为你选择 到底应该选择 一个二次函数 或者一个三次函数 还是别的函数 但是 在我们 学到那种算法之前 现在我希望你知道 你需要选择 使用什么特征 并且通过设计不同的特征 你能够用更复杂的函数 去拟合你的数据 而不是只用 一条直线去拟合 特别是 你也可以使用多项式 函数 有时候 通过采取适当的角度来观察 特征就可以 得到一个更符合你的数据的模型
### 内置习题_多项式回归特征缩放问题
![内置习题_多项式回归特征缩放问题](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/25-内置习题_多项式回归特征缩放问题.jpg)
## English
### Example for what is Polynomial Regression
![举例说明什么是多项式回归](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/22-举例说明什么是多项式回归.jpg)
You now know about linear regression with multiple variables. In this video, I wanna tell you a bit about the choice of features that you have and how you can get different learning algorithm, sometimes very powerful ones by choosing appropriate features. And in particular I also want to tell you about polynomial regression allows you to use the machinery of linear regression to fit very complicated, even very non-linear functions. Let's take the example of predicting the price of the house. Suppose you have two features, the frontage of house and the depth of the house. So, here's the picture of the house we're trying to sell. So, the frontage is defined as this distance is basically the width or the length of how wide your lot is if this that you own, and the depth of the house is how deep your property is, so there's a frontage, there's a depth. called frontage and depth. You might build a linear regression model like this where frontage is your first feature x1 and and depth is your second feature x2, but when you're applying linear regression, you don't necessarily have to use just the features x1 and x2 that you're given. What you can do is actually create new features by yourself. So, if I want to predict the price of a house, what I might do instead is decide that what really determines the size of the house is the area or the land area that I own. So, I might create a new feature. I'm just gonna call this feature x which is frontage, times depth. This is a multiplication symbol. It's a frontage x depth because this is the land area that I own and I might then select my hypothesis as that using just one feature which is my land area, right? Because the area of a rectangle is you know, the product of the length of the size So, depending on what insight you might have into a particular problem, rather than just taking the features [xx] that we happen to have started off with, sometimes by defining new features you might actually get a better model. Closely related to the idea of choosing your features is this idea called polynomial regression.
### Example for Polynomial Regression and note that feature scaling problem
![Example for Polynomial Regression and note that feature scaling problem](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/23-举例说明多项式回归以及注意特征缩放问题.jpg)
Let's say you have a housing price data set that looks like this. Then there are a few different models you might fit to this. One thing you could do is fit a quadratic model like this. It doesn't look like a straight line fits this data very well. So maybe you want to fit a quadratic model like this where you think the size, where you think the price is a quadratic function and maybe that'll give you, you know, a fit to the data that looks like that. But then you may decide that your quadratic model doesn't make sense because of a quadratic function, eventually this function comes back down and well, we don't think housing prices should go down when the size goes up too high. So then maybe we might choose a different polynomial model and choose to use instead a cubic function, and where we have now a third-order term and we fit that, maybe we get this sort of model, and maybe the green line is a somewhat better fit to the data cause it doesn't eventually come back down. So how do we actually fit a model like this to our data? Using the machinery of multivariant linear regression, we can do this with a pretty simple modification to our algorithm. The form of the hypothesis we, we know how the fit looks like this, where we say H of x is theta zero plus theta one x one plus x two theta X3. And if we want to fit this cubic model that I have boxed in green, what we're saying is that to predict the price of a house, it's theta 0 plus theta 1 times the size of the house plus theta 2 times the square size of the house. So this term is equal to that term. And then plus theta 3 times the cube of the size of the house raises that third term. In order to map these two definitions to each other, well, the natural way to do that is to set the first feature x one to be the size of the house, and set the second feature x two to be the square of the size of the house, and set the third feature x three to be the cube of the size of the house. And, just by choosing my three features this way and applying the machinery of linear regression, I can fit this model and end up with a cubic fit to my data. I just want to point out one more thing, which is that if you choose your features like this, then feature scaling becomes increasingly important. So if the size of the house ranges from one to a thousand, so, you know, from one to a thousand square feet, say, then the size squared of the house will range from one to one million, the square of a thousand, and your third feature x cubed, excuse me you, your third feature x three, which is the size cubed of the house, will range from one two ten to the nine, and so these three features take on very different ranges of values, and it's important to apply feature scaling if you're using gradient descent to get them into comparable ranges of values.
### Example for Polynomial Regression about features choices
![Example for Polynomial Regression about features choices](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/24-举例说明如何进行特征选择.jpg)
Finally, here's one last example of how you really have broad choices in the features you use. Earlier we talked about how a quadratic model like this might not be ideal because, you know, maybe a quadratic model fits the data okay, but the quadratic function goes back down and we really don't want, right, housing prices that go down, to predict that, as the size of housing freezes. But rather than going to a cubic model there, you have, maybe, other choices of features and there are many possible choices. But just to give you another example of a reasonable choice, another reasonable choice might be to say that the price of a house is theta zero plus theta one times the size, and then plus theta two times the square root of the size, right? So the square root function is this sort of function, and maybe there will be some value of theta one, theta two, theta three, that will let you take this model and, for the curve that looks like that, and, you know, goes up, but sort of flattens out a bit and doesn't ever come back down. And, so, by having insight into, in this case, the shape of a square root function, and, into the shape of the data, by choosing different features, you can sometimes get better models. In this video, we talked about polynomial regression. That is, how to fit a polynomial, like a quadratic function, or a cubic function, to your data. Was also throw out this idea, that you have a choice in what features to use, such as that instead of using the frontish and the depth of the house, maybe, you can multiply them together to get a feature that captures the land area of a house. In case this seems a little bit bewildering, that with all these different feature choices, so how do I decide what features to use. Later in this class, we'll talk about some algorithms were automatically choosing what features are used, so you can have an algorithm look at the data and automatically choose for you whether you want to fit a quadratic function, or a cubic function, or something else. But, until we get to those algorithms now I just want you to be aware that you have a choice in what features to use, and by designing different features you can fit more complex functions your data then just fitting a straight line to the data and in particular you can put polynomial functions as well and sometimes by appropriate insight into the feature simply get a much better model for your data. 
### Exam_the problem about feature scaling when use Polynomial Regression
![Exam_the problem about feature scaling when use Polynomial Regression](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/25-内置习题_多项式回归特征缩放问题.jpg)
