# 神经网络学习- 实现注意：优化算法中参数矩阵展开成向量
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/05-Week5/1-Neural Networks Learning/4-Implementation Note_Unrolling Parameters.mp4" type="video/mp4">
</video>
## 中文
### 反向传播神经网络高级优化算法将参数矩阵展开成向量
![反向传播神经网络高级优化算法将参数矩阵展开成向量](amWiki/images/001/05-Week5/1-Neural Networks Learning/17-反向传播神经网络高级优化算法将参数矩阵展开成向量.jpg)  
在上一段视频中 我们谈到了怎样使用反向传播算法计算代价函数的导数 在这段视频中 我想快速地向你介绍一个细节的实现过程 怎样把你的参数 从矩阵展开成向量 以便我们在高级最优化步骤中的使用需要 具体来讲 你执行了代价函数costFunction 输入参数是theta 函数返回值是代价函数以及导数值 然后你可以将返回值 传递给高级最优化算法fminunc 顺便提醒 fminunc并不是唯一的算法 你也可以使用别的优化算法 但它们的功能 都是取出这些输入值 @costFunction 以及theta值的一些初始值 并且这些程序 都假设theta 和这些theta初始值 都是参数向量 也许是n或者n+1阶 但它们都是向量 同时假设这个代价函数 第二个返回值 也就是gradient值 也是n阶或者n+1阶 所以它也是一个向量 这部分在我们使用逻辑回归的时候 运行顺利 但现在 对于神经网络 我们的参数将不再是 向量 而是矩阵了 因此对于一个完整的神经网络 我们的参数矩阵为θ(1) θ(2) θ(3) 在Octave中我们可以设为 Theta1 Theta2 Theta3 类似的 这些梯度项gradient 也是需要得到的返回值 那么在之前的视频中 我们演示了如何计算 这些梯度矩阵 它们是D(1) D(2) D(3) 在Octave中 我们用矩阵D1 D2 D3来表示
### 举例说明参数矩阵展开成向量过程
![举例说明参数矩阵展开成向量过程](amWiki/images/001/05-Week5/1-Neural Networks Learning/18-举例说明参数矩阵展开成向量过程.jpg)  
在这节视频中 我想很快地向你介绍 怎样取出这些矩阵 并且将它们展开成向量 以便它们最终 成为恰当的格式 能够传入这里的Theta 并且得到正确的梯度返回值gradient 具体来说 假设我们有这样一个神经网络 其输入层有10个输入单元 隐藏层有10个单元 最后的输出层 只有一个输出单元 因此s1等于第一层的单元数 s2等于第二层的单元数 s3等于第三层的 单元个数 在这种情况下 矩阵θ的维度 和矩阵D的维度 将由这些表达式确定 比如说 θ(1)是一个10x11的矩阵 以此类推 因此 在Octave中 如果你想将这些矩阵 转化为向量 那么你要做的 是取出你的Theta1 Theta2 Theta3 然后使用这段代码 这段代码将取出 三个θ矩阵中的所有元素 也就是说取出Theta1 的所有元素 Theta2的所有元素 Theta3的所有元素 然后把它们全部展开 成为一个很长的向量 也就是thetaVec 同样的 第二段代码 将取出D矩阵的所有元素 然后展开 成为一个长向量 被叫做DVec 最后 如果你想从向量表达 返回到矩阵表达式的话 你要做的是 比如想再得到Theta1 那么取thetaVec 抽出前110个元素 因此 Theta1就有110个元素 因为它应该是一个10x11的矩阵 所以 抽出前110个元素 然后你就可以 reshape矩阵变维命令来重新得到Theta1 同样类似的 要重新得到Theta2矩阵 你需要抽出下一组110个元素并且重新组合 然后对于Theta3 你需要抽出最后11个元素 然后执行reshape命令 重新得到Theta3 以下是这一过程的Octave演示 对于这一个例子 让我们假设Theta1 为一个10x11的单位矩阵 因此它每一项都为1 为了更易看清 让我们把Theta2设为 一个10行11列矩阵 每个元素都为2 然后设Theta3 是一个1x11的矩阵 每个元素都为3 因此 这样我们得到三个独立的矩阵 Theta1 Theta2 Theta3 现在我们想把所有这些矩阵变成一个向量 thetaVec = [Theta1(:); Theta2(:); Theta3(:)]; 好的 注意中间有冒号 像这样 现在thetaVec矩阵 就变成了一个很长的向量 含有231个元素 如果把它打出来 我们就能看出它是一个很长的向量 包括第一个矩阵的所有元素 第二个矩阵的所有元素 以及第三个矩阵的所有元素 如果我想重新得到 我最初的三个矩阵 我可以对thetaVec使用reshape命令 抽出前110个元素 将它们重组为一个10x11的矩阵 这样我又再次得到了Theta1矩阵 然后我再取出 接下来的110个元素 也就是111到220号元素 我就又重组还原了第二个矩阵 最后 再抽出221到最后一个元素 也就是第231个元素 然后重组为1x11的矩阵 我就又得到了Theta3矩阵
### 将参数矩阵展开成向量方法应用于反向传播学习算法
![将参数矩阵展开成向量方法应用于反向传播学习算法](amWiki/images/001/05-Week5/1-Neural Networks Learning/19-将参数矩阵展开成向量方法应用于反向传播学习算法.jpg)  
为了使这个过程更形象 下面我们来看怎样将这一方法 应用于我们的学习算法 假设说你有一些 初始参数值 θ(1) θ(2) θ(3) 我们要做的是 取出这些参数并且将它们 展开为一个长向量 我们称之为initialTheta 然后作为theta参数的初始设置 传入函数fminunc 我们要做的另一件事是执行代价函数costFunction 实现算法如下 代价函数costFunction 将传入参数thetaVec 这也是包含 我所有参数的向量 是将所有的参数展开成一个向量的形式 因此我要做的第一件事是 我要使用 thetaVec和重组函数reshape 因此我要抽出thetaVec中的元素 然后重组 以得到我的初始参数矩阵 θ(1) θ(2) θ(3) 所以这些是我需要得到的矩阵 因此 这样我就有了 一个使用这些矩阵的 更方便的形式 这样我就能执行前向传播 和反向传播 来计算出导数 以求得代价函数的J(θ) 最后 我可以取出这些导数值 然后展开它们 让它们保持和我展开的θ值 同样的顺序 我要展开D1 D2 D3 来得到gradientVec 这个值可由我的代价函数返回 它可以以一个向量的形式返回这些导数值 现在 我想 对怎样进行参数的矩阵表达式 和向量表达式 之间的转换 有了一个更清晰的认识 使用矩阵表达式 的好处是 当你的参数以矩阵的形式储存时 你在进行正向传播 和反向传播时 你会觉得更加方便 当你将参数储存为矩阵时 一大好处是 充分利用了向量化的实现过程 相反地 向量表达式的优点是 如果你有像thetaVec或者DVec这样的矩阵 当你使用一些高级的优化算法时 这些算法通常要求 你所有的参数 都要展开成一个长向量的形式 希望通过我们刚才介绍的内容 你能够根据需要 更加轻松地 在两种形式之间转换【果壳教育无边界字幕组】翻译：所罗门捷列夫 校对: Roy薛
### 内置习题
![内置习题_理解参数矩阵展开成向量](amWiki/images/001/05-Week5/1-Neural Networks Learning/20-内置习题_理解参数矩阵展开成向量.jpg)
## English
### BackPropagation neural network of Advanced Optimization Algorithms unroll parameter matrix  into vector
![BackPropagation neural network of Advanced Optimization Algorithms unroll parameter matrix  into vector](amWiki/images/001/05-Week5/1-Neural Networks Learning/17-反向传播神经网络高级优化算法将参数矩阵展开成向量.jpg)  
In the previous video, we talked about how to use back propagation to compute the derivatives of your cost function. In this video, I want to quickly tell you about one implementational detail of unrolling your parameters from matrices into vectors, which we need in order to use the advanced optimization routines.Concretely, let's say you've implemented a cost function that takes this input, you know, parameters theta and returns the cost function and returns derivatives.Then you can pass this to an advanced authorization algorithm by fminunc and fminunc isn't the only one by the way. There are also other advanced authorization algorithms.But what all of them do is take those input pointedly the cost function, and some initial value of theta. And both, and these routines assume that theta and the initial value of theta, that these are parameter vectors, maybe Rn or Rn plus 1. But these are vectors and it also assumes that, you know, your cost function will return as a second return value this gradient which is also Rn and Rn plus 1. So also a vector. This worked fine when we were using logistic progression but now that we're using a neural network our parameters are no longer vectors, but instead they are these matrices where for a full neural network we would have parameter matrices theta 1, theta 2, theta 3 that we might represent in Octave as these matrices theta 1, theta 2, theta 3. And similarly these gradient terms that were expected to return. Well, in the previous video we showed how to compute these gradient matrices, which was capital D1, capital D2, capital D3, which we might represent an octave as matrices D1, D2, D3.
### Illustrate the parameter matrix unroll to a vector process
![Illustrate the parameter matrix unroll to a vector process](amWiki/images/001/05-Week5/1-Neural Networks Learning/18-举例说明参数矩阵展开成向量过程.jpg)  
In this video I want to quickly tell you about the idea of how to take these matrices and unroll them into vectors. So that they end up being in a format suitable for passing into as theta here off for getting out for a gradient there.Concretely, let's say we have a neural network with one input layer with ten units, hidden layer with ten units and one output layer with just one unit, so s1 is the number of units in layer one and s2 is the number of units in layer two, and s3 is a number of units in layer three. In this case, the dimension of your matrices theta and D are going to be given by these expressions. For example, theta one is going to a 10 by 11 matrix and so on.So in if you want to convert between these matrices. vectors. What you can do is take your theta 1, theta 2, theta 3, and write this piece of code and this will take all the elements of your three theta matrices and take all the elements of theta one, all the elements of theta 2, all the elements of theta 3, and unroll them and put all the elements into a big long vector.Which is thetaVec and similarly the second command would take all of your D matrices and unroll them into a big long vector and call them DVec. And finally if you want to go back from the vector representations to the matrix representations.What you do to get back to theta one say is take thetaVec and pull out the first 110 elements. So theta 1 has 110 elements because it's a 10 by 11 matrix so that pulls out the first 110 elements and then you can use the reshape command to reshape those back into theta 1. And similarly, to get back theta 2 you pull out the next 110 elements and reshape it. And for theta 3, you pull out the final eleven elements and run reshape to get back the theta 3.Here's a quick Octave demo of that process. So for this example let's set theta 1 equal to be ones of 10 by 11, so it's a matrix of all ones. And just to make this easier seen, let's set that to be 2 times ones, 10 by 11 and let's set theta 3 equals 3 times 1's of 1 by 11. So this is 3 separate matrices: theta 1, theta 2, theta 3. We want to put all of these as a vector. ThetaVec equals theta 1; theta 2 theta 3. Right, that's a colon in the middle and like so and now thetavec is going to be a very long vector. That's 231 elements.If I display it, I find that this very long vector with all the elements of the first matrix, all the elements of the second matrix, then all the elements of the third matrix.And if I want to get back my original matrices, I can do reshape thetaVec.Let's pull out the first 110 elements and reshape them to a 10 by 11 matrix.This gives me back theta 1. And if I then pull out the next 110 elements. So that's indices 111 to 220. I get back all of my 2's.And if I go from 221 up to the last element, which is element 231, and reshape to 1 by 11, I get back theta 3.
### The parameter matrix unroll into a vector method is applied to the back propagation learning algorithm
![The parameter matrix unroll into a vector method is applied to the back propagation learning algorithm](amWiki/images/001/05-Week5/1-Neural Networks Learning/19-将参数矩阵展开成向量方法应用于反向传播学习算法.jpg)  
To make this process really concrete, here's how we use the unrolling idea to implement our learning algorithm.Let's say that you have some initial value of the parameters theta 1, theta 2, theta 3. What we're going to do is take these and unroll them into a long vector we're gonna call initial theta to pass in to fminunc as this initial setting of the parameters theta.The other thing we need to do is implement the cost function.Here's my implementation of the cost function.The cost function is going to give us input, thetaVec, which is going to be all of my parameters vectors that in the form that's been unrolled into a vector.So the first thing I'm going to do is I'm going to use thetaVec and I'm going to use the reshape functions. So I'll pull out elements from thetaVec and use reshape to get back my original parameter matrices, theta 1, theta 2, theta 3. So these are going to be matrices that I'm going to get. So that gives me a more convenient form in which to use these matrices so that I can run forward propagation and back propagation to compute my derivatives, and to compute my cost function j of theta.And finally, I can then take my derivatives and unroll them, to keeping the elements in the same ordering as I did when I unroll my thetas. But I'm gonna unroll D1, D2, D3, to get gradientVec which is now what my cost function can return. It can return a vector of these derivatives. So, hopefully, you now have a good sense of how to convert back and forth between the matrix representation of the parameters versus the vector representation of the parameters.The advantage of the matrix representation is that when your parameters are stored as matrices it's more convenient when you're doing forward propagation and back propagation and it's easier when your parameters are stored as matrices to take advantage of the, sort of, vectorized implementations.Whereas in contrast the advantage of the vector representation, when you have like thetaVec or DVec is that when you are using the advanced optimization algorithms. Those algorithms tend to assume that you have all of your parameters unrolled into a big long vector. And so with what we just went through, hopefully you can now quickly convert between the two as needed.
### Exam_in the video
![Exam_Understand Backpropagation algorithm](amWiki/images/001/05-Week5/1-Neural Networks Learning/20-内置习题_理解参数矩阵展开成向量.jpg)
