# 神经网络表达-模型表达I
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/04-Week4/1-Neural Networks Representation/3-Model Representation I.mp4" type="video/mp4">
</video>
## 中文
### 人脑中神经元介绍
![人脑中神经元介绍](amWiki/images/001/04-Week4/1-Neural Networks Representation/12-人脑中神经元介绍.jpg)  
在这个视频中 我想 开始向你介绍 我们该如何表示神经网络 换句话说 当我们在 运用神经网络时 我们该如何表示我们的假设或模型 神经网络是在模仿 大脑中的神经元或者神经网络时发明的 因此 要解释如何表示 模型假设 我们先来看单个 神经元在大脑中 是什么样的我们的大脑中充满了 这样的神经元 神经元是大脑中的细胞 其中有两点 值得我们注意 一是神经元有 像这样的细胞主体 二是神经元有 一定数量的 输入神经 这些输入神经叫做树突 可以把它们想象成输入电线 它们接收来自其他 神经元的信息 神经元的输出神经叫做轴突 这些输出神经 是用来 给其他神经元传递信号 或者传送信息的 简而言之 神经元是一个计算单元 它从输入神经接受一定数目的信息 并做一些计算 然后将结果通过它的 轴突传送到其他节点 或者大脑中的其他神经元
### 人脑中单个神经元信息传递介绍
![人脑中单个神经元信息传递介绍](amWiki/images/001/04-Week4/1-Neural Networks Representation/13-人脑中单个神经元信息传递介绍.jpg)  
下面是一组神经元的示意图 神经元利用微弱的电流 进行沟通 这些弱电流也称作动作电位 其实就是一些微弱的电流 所以如果 神经元想要 传递一个消息 它就会就通过它的轴突 发送一段微弱电流 给其他神经元 这就是轴突 这里是一条 连接到输入神经 或者连接另一个神经元 树突的神经 接下来这个神经元接收这条消息 做一些计算 它有可能会反过来将 在轴突上的 自己的消息传给其他神经元 这就是所有 人类思考的模型： 我们的神经元把 自己的收到的消息进行计算 并向其他神经元 传递消息 顺便说一下 这也是 我们的感觉和肌肉运转的原理 如果你想活动一块肌肉 就会触发一个神经元 给你的肌肉 发送脉冲 并引起 你的肌肉收缩 如果一些感官 比如说眼睛 想要给大脑传递 一个消息 那么它就像这样发送 电脉冲给大脑的
### 人工神经网络模型介绍
![人工神经网络模型介绍](amWiki/images/001/04-Week4/1-Neural Networks Representation/14-人工神经网络模型介绍.jpg)
在一个神经网络里 或者说在我们在电脑上 实现的人工神经网络里 我们将使用 一个非常简单的模型 来模拟神经元的工作 我们将神经元模拟成一个逻辑单元 当我画一个这样的 黄色圆圈时 你应该 把它想象成 作用类似于 神经元的东西 然后我们通过 它的树突或者说它的输入神经 传递给它一些信息 然后神经元做一些计算并通过它的输出神经 即它的轴突 输出计算结果 当我画一个 像这样的图表时 就表示对h(x)的计算 h(x)等于1除以1加e的负θ转置乘以 x 通常 x和θ 是我们的参数向量 这是一个简单的模型 甚至说是一个过于简单的 模拟神经元的模型 它被输入 x1 x2和 x3 然后输出一些 类似这样的结果 当我绘制一个神经网络时 通常我只绘制 输入节点 x1 x2 x3 但有时也可以这样做： 我增加一个额外的节点 x0 这个 x0 节点 有时也被称作偏置单位 或偏置神经元 但因为 x0 总是等于1 所以有时候 我会画出它 有时我不会画出 这取决于它是否对例子有利 现在来讨论 最后一个关于 神经网络的术语 有时我们会说 这是一个神经元 一个有s型函数或者逻辑函数作为激励函数的 人工神经元 在神经网络术语中 激励函数只是对类似非线性 函数g(z)的另一个术语称呼 g(z)等于 1除以1 加e的-z次方 到目前为止 我一直称θ为 模型的参数 以后大概会继续将这个术语与 “参数”相对应 而不是与神经网络 在关于神经网络的文献里 有时你可能会看到人们 谈论一个模型的权重 权重其实和 模型的参数 是一样的东西 在视频中我会继续使用“参数”这个术语 但有时你可能听到别人用“权重”这个术语 这个小圈 代表一个单一的神经元 神经网络其实就是 这些不同的神经元 组合在一起的集合 具体来说 这里是我们的 输入单元 x1 x2和 x3 再说一次 有时也可以画上 额外的节点 x0 我把 x0 画在这了这里有 3个神经元 我在里面写了a(2)1 a(2)2 和a(2)3 然后再次说明 我们可以在这里 添加一个a0 和一个额外的偏度单元 它的值永远是1 最后 我们在 最后一层有第三个节点 正是这第三个节点 输出 假设函数h(x)计算的结果
### 人工神经网络中的输入、隐藏、输出层介绍
![人工神经网络中的输入、隐藏、输出层介绍](amWiki/images/001/04-Week4/1-Neural Networks Representation/15-人工神经网络中的输入、隐藏、输出层介绍.jpg)
再多说一点关于 神经网络的术语 网络中的第一层 也被称为输入层 因为我们在这一层 输入我们的特征项 x1 x2 x3 最后一层 也称为输出层 因为这一层的 神经元—我指的这个 输出 假设的最终计算结果 中间的两层 也被称作隐藏层 隐藏层不是一个 很合适的术语 但是 直觉上我们知道 在监督学习中 你能看到输入 也能看到正确的输出 而隐藏层的值 你在训练集里是看不到的 它的值不是 x 也不是y 所以我们叫它隐藏层 稍后我们会看到神经网络 可以有不止一个的 隐藏层 但在 这个例子中 我们有一个 输入层—第1层 一个隐藏层— 第2层 和一个输出层—第3层 但实际上任何 非输入层或非输出层的层 就被称为隐藏层
### 人工神经网络计算过程分析
![人工神经网络计算过程分析](amWiki/images/001/04-Week4/1-Neural Networks Representation/16-人工神经网络计算过程分析.jpg)
接下来 我希望你们明白神经网络 究竟在做什么 让我们逐步分析 这个图表所呈现的 计算步骤 为了解释这个神经网络 具体的计算步骤 这里还有些记号要解释 我要使用a上标(j) 下标i表示 第j层的 第i个神经元或单元 具体来说 这里a上标(2) 下标1 表示第2层的 第一个激励 即隐藏层的第一个激励 所谓激励(activation) 是指 由一个具体神经元读入 计算并输出的值 此外 我们的神经网络 被这些矩阵参数化 θ上标(j) 它将成为 一个波矩阵 控制着 从一层 比如说从第一层 到第二层或者第二层到第三层的作用 所以 这就是这张图所表示的计算 这里的第一个隐藏单元是这样计算它的值的： a(2)1等于 s函数 或者说s激励函数 也叫做逻辑激励函数 作用在这种 输入的线性组合上的结果 第二个隐藏单元 等于s函数作用在这个 线性组合上的值 同样 对于第三个 隐藏的单元 它是通过这个公式计算的 在这里 我们有三个 输入单元和三个隐藏单元 这样一来 参数矩阵控制了 我们来自 三个输入单元 三个隐藏单元的映射 因此θ1的维数将变成3 θ1将变成一个 3乘4维的 矩阵 更一般的如果一个网络在第j 层有sj个单元 在j+1层有 sj+1个单元 那么矩阵θ(j) 即控制第j层到 第j+1层映射 的矩阵的 维度为s(j+1) * (sj+1) 这里要搞清楚 这个是s下标j+1 而这个是 s下标j 然后 整体加上1 整体加1 明白了吗 所以θ(j)的维度是 s(j+1)行 sj+1列 这里sj+1 当中的1 不是下标的一部分 以上我们讨论了 三个隐藏单位是怎么计算它们的值 最后 在输出层 我们还有一个 单元 它计算 h(x) 这个也可以 写成a(3)1 就等于后面这块 注意到我这里 写了个上标2 因为θ上标2 是参数矩阵 或着说是权重矩阵 该矩阵 控制从第二层 即隐藏层的3个单位 到第三层 的一个单元 即输出单元的映射 总之 以上我们 展示了像这样一张图是 怎样定义 一个人工神经网络的 这个神经网络定义了函数h： 从输入 x 到输出y的映射 我将这些假设的参数 记为大写的θ 这样一来 不同的θ 对应了不同的假设 所以我们有不同的函数 比如说从 x到y的映射 以上就是我们怎么 从数学上定义 神经网络的假设在接下来的视频中 我想要做的就是 让你对这些假设的作用 有更深入的理解 并且讲解几个例子 然后谈谈如何有效的计算它们
### 内置习题
![内置习题_非线性分类举例](amWiki/images/001/04-Week4/1-Neural Networks Representation/7-内置习题_非线性分类举例.jpg)
## English
### Example for Non-linear Classification
![Example for Non-linear Classification](amWiki/images/001/04-Week4/1-Neural Networks Representation/12-人脑中神经元介绍.jpg)
In this video, I want to start telling you about how we represent neural networks. In other words, how we represent our hypothesis or how we represent our model when using neural networks. Neural networks were developed as simulating neurons or networks of neurons in the brain. So, to explain the hypothesis representation let's start by looking at what a single neuron in the brain looks like. Your brain and mine is jam packed full of neurons like these and neurons are cells in the brain. And two things to draw attention to are that first. The neuron has a cell body, like so, and moreover, the neuron has a number of input wires, and these are called the dendrites. You think of them as input wires, and these receive inputs from other locations. And a neuron also has an output wire called an Axon, and this output wire is what it uses to send signals to other neurons, so to send messages to other neurons. So, at a simplistic level what a neuron is, is a computational unit that gets a number of inputs through it input wires and does some computation and then it says outputs via its axon to other nodes or to other neurons in the brain.
### Neurons communicate in the brain
![Neurons communicate in the brain](amWiki/images/001/04-Week4/1-Neural Networks Representation/13-人脑中单个神经元信息传递介绍.jpg)
Here's a illustration of a group of neurons. The way that neurons communicate with each other is with little pulses of electricity, they are also called spikes but that just means pulses of electricity. So here is one neuron and what it does is if it wants a send a message what it does is sends a little pulse of electricity. Varis axon to some different neuron and here, this axon that is this open wire, connects to the dendrites of this second neuron over here, which then accepts this incoming message that some computation. And they, in turn, decide to send out this message on this axon to other neurons, and this is the process by which all human thought happens. It's these Neurons doing computations and passing messages to other neurons as a result of what other inputs they've got. And, by the way, this is how our senses and our muscles work as well. If you want to move one of your muscles the way that where else in your neuron may send this electricity to your muscle and that causes your muscles to contract and your eyes, some senses like your eye must send a message to your brain while it does it senses hosts electricity entity to a neuron in your brain like so.
###  Artificial Neurons Networks Mode Introduction
![Artificial Neurons Networks Mode Introduction](amWiki/images/001/04-Week4/1-Neural Networks Representation/14-人工神经网络模型介绍.jpg)
In a neuro network, or rather, in an artificial neuron network that we've implemented on the computer, we're going to use a very simple model of what a neuron does we're going to model a neuron as just a logistic unit. So, when I draw a yellow circle like that, you should think of that as a playing a role analysis, who's maybe the body of a neuron, and we then feed the neuron a few inputs who's various dendrites or input wiles.And the neuron does some computation. And output some value on this output wire, or in the biological neuron, this is an axon. And whenever I draw a diagram like this, what this means is that this represents a computation of h of x equals one over one plus e to the negative theta transpose x, where as usual, x and theta are our parameter vectors, like so.So this is a very simple, maybe a vastly oversimplified model, of the computations that the neuron does, where it gets a number of inputs, x1, x2, x3 and it outputs some value computed like so.When I draw a neural network, usually I draw only the input nodes x1, x2, x3. Sometimes when it's useful to do so, I'll draw an extra node for x0.This x0 now that's sometimes called the bias unit or the bias neuron, but because x0 is already equal to 1, sometimes, I draw this, sometimes I won't just depending on whatever is more notationally convenient for that example.Finally, one last bit of terminology when we talk about neural networks, sometimes we'll say that this is a neuron or an artificial neuron with a Sigmoid or logistic activation function. So this activation function in the neural network terminology. This is just another term for that function for that non-linearity g(z) = 1 over 1+e to the -z. And whereas so far I've been calling theta the parameters of the model, I'll mostly continue to use that terminology. Here, it's a copy to the parameters, but in neural networks, in the neural network literature sometimes you might hear people talk about weights of a model and weights just means exactly the same thing as parameters of a model. But I'll mostly continue to use the terminology parameters in these videos, but sometimes, you might hear others use the weights terminology.So, this little diagram represents a single neuron.What a neural network is, is just a group of this different neurons strong together. Completely, here we have input units x1, x2, x3 and once again, sometimes you can draw this extra note x0 and Sometimes not, just flow that in here. And here we have three neurons which have written 81, 82, 83. I'll talk about those indices later. And once again we can if we want add in just a0 and add the mixture bias unit there. There's always a value of 1. And then finally we have this third node and the final layer, and there's this third node that outputs the value that the hypothesis h(x) computes.
###  Artificial Neurons Networks of input、hidden、output layer
![Artificial Neurons Networks of input、hidden、output layer](amWiki/images/001/04-Week4/1-Neural Networks Representation/15-人工神经网络中的输入、隐藏、输出层介绍.jpg)
To introduce a bit more terminology, in a neural network, the first layer, this is also called the input layer because this is where we Input our features, x1, x2, x3. The final layer is also called the output layer because that layer has a neuron, this one over here, that outputs the final value computed by a hypothesis. And then, layer 2 in between, this is called the hidden layer. The term hidden layer isn't a great terminology, but this ideation is that, you know, you supervised early, where you get to see the inputs and get to see the correct outputs, where there's a hidden layer of values you don't get to observe in the training setup. It's not x, and it's not y, and so we call those hidden. And they try to see neural nets with more than one hidden layer but in this example, we have one input layer, Layer 1, one hidden layer, Layer 2, and one output layer, Layer 3. But basically, anything that isn't an input layer and isn't an output layer is called a hidden layer.
### The computational steps of  Artificial Neurons Networks Analysis
![The computational steps of  Artificial Neurons Networks Analysis](amWiki/images/001/04-Week4/1-Neural Networks Representation/16-人工神经网络计算过程分析.jpg)
So I want to be really clear about what this neural network is doing. Let's step through the computational steps that are and body represented by this diagram. To explain these specific computations represented by a neural network, here's a little bit more notation. I'm going to use a superscript j subscript i to denote the activation of neuron i or of unit i in layer j. So completely this gave superscript to sub group one, that's the activation of the first unit in layer two, in our hidden layer. And by activation I just mean the value that's computed by and as output by a specific. In addition, new network is parametrize by these matrixes, theta super script j Where theta j is going to be a matrix of weights controlling the function mapping form one layer, maybe the first layer to the second layer, or from the second layer to the third layer.So here are the computations that are represented by this diagram.This first hidden unit here has it's value computed as follows, there's a is a21 is equal to the sigma function of the sigma activation function, also called the logistics activation function, apply to this sort of linear combination of these inputs. And then this second hidden unit has this activation value computer as sigmoid of this. And similarly for this third hidden unit is computed by that formula. So here we have 3 theta 1 which is matrix of parameters governing our mapping from our three different units, our hidden units. Theta 1 is going to be a 3.Theta 1 is going to be a 3x4-dimensional matrix. And more generally, if a network has SJU units in there j and sj + 1 units and sj + 1 then the matrix theta j which governs the function mapping from there sj + 1. That will have to mention sj +1 by sj + 1 I'll just be clear about this notation right. This is Subscript j + 1 and that's s subscript j, and then this whole thing, plus 1, this whole thing (sj + 1), okay? So that's s subscript j + 1 by,So that's s subscript j + 1 by sj + 1 where this plus one is not part of the subscript. Okay, so we talked about what the three hidden units do to compute their values. Finally, there's a loss of this final and after that we have one more unit which computer h of x and that's equal can also be written as a(3)1 and that's equal to this. And you notice that I've written this with a superscript two here, because theta of superscript two is the matrix of parameters, or the matrix of weights that controls the function that maps from the hidden units, that is the layer two units to the one layer three unit, that is the output unit. To summarize, what we've done is shown how a picture like this over here defines an artificial neural network which defines a function h that maps with x's input values to hopefully to some space that provisions y. And these hypothesis are parameterized by parameters denoting with a capital theta so that, as we vary theta, we get different hypothesis and we get different functions. Mapping say from x to y.So this gives us a mathematical definition of how to represent the hypothesis in the neural network. In the next few videos what I would like to do is give you more intuition about what these hypothesis representations do, as well as go through a few examples and talk about how to compute them efficiently.
### Exam_in the video
![Exam_Calculate the parameter theta【weights】](amWiki/images/001/04-Week4/1-Neural Networks Representation/17-内置习题_人工神经网络参数【权重】计算.jpg)
